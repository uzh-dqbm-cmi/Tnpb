{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a65c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201bb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "from models.data_process import get_datatensor_partitions, prepare_nonproto_features, generate_partition_datatensor,get_data_ready\n",
    "from models.dataset import ProtospacerDataset, ProtospacerExtendedDataset\n",
    "from models.trainval_workflow import run_trainevaltest_workflow\n",
    "from models.trainval_workflow import run_inference\n",
    "from models.hyperparam import build_config_map\n",
    "from src.utils import create_directory, one_hot_encode, get_device, ReaderWriter \n",
    "from src.utils import print_eval_results, plot_y_distrib_acrossfolds, compute_eval_results_df\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f581bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_opt = argparse.ArgumentParser(description='Argparser for data')\n",
    "cmd_opt.add_argument('-model_name',  type=str, help = 'name of the model')\n",
    "cmd_opt.add_argument('-exp_name',  type=str, help = 'name of the experiment')\n",
    "\n",
    "cmd_opt.add_argument('-data_dir',  type=str,default = './data/', help = 'directory of the data')\n",
    "cmd_opt.add_argument('-target_dir',  type=str, default='processed',  help = 'folder name to save the processed data')\n",
    "cmd_opt.add_argument('-working_dir',  type=str, default='./', help = 'the main working directory')\n",
    "cmd_opt.add_argument('-output_path', type=str, help='path to save the trained model')\n",
    "cmd_opt.add_argument('-random_seed', type=int,default=42)\n",
    "cmd_opt.add_argument('-epoch_num', type=int, default =200, help='number of training epochs')\n",
    "args, _ = cmd_opt.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8dbdc3",
   "metadata": {},
   "source": [
    "### Functions to make data ready and get predefined hyperparams for a given model and experiment choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcab8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined hyperparameters depending on the chosen model and experiment\n",
    "def get_hyperparam_config(args):\n",
    "    \"return predefined hyperparameters for each model\"\n",
    "    to_gpu = True\n",
    "    gpu_index=0\n",
    "    optim_tup = None\n",
    "    \n",
    "    if args.model_name == 'FFN':\n",
    "        batch_size = 100\n",
    "        num_epochs = 300\n",
    "        h = [60,10]\n",
    "        l2_reg =0.1\n",
    "        model_config_tup = (h, l2_reg, batch_size, num_epochs)\n",
    "        \n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.ReLU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20\n",
    "        \n",
    "        loss_func_name = 'MSEloss'\n",
    "        perfmetric_name = 'pearson'\n",
    "        \n",
    "    if args.model_name == 'CNN':\n",
    "        k = 2\n",
    "        l2_reg = 0.5\n",
    "        batch_size = 100\n",
    "        num_epochs = 300\n",
    "        model_config_tup = (k, l2_reg, batch_size, num_epochs)\n",
    "\n",
    "\n",
    "        # input_dim, embed_dim, mlp_embed_factor, nonlin_func, p_dropout, num_encoder_units\n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.ReLU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20\n",
    "\n",
    "        loss_func_name = 'MSEloss'\n",
    "        # loss_func_name = 'SmoothL1loss'\n",
    "        perfmetric_name = 'spearman'\n",
    "\n",
    "    elif args.model_name == 'RNN':\n",
    "        embed_dim = 64\n",
    "        hidden_dim = 64\n",
    "        z_dim = 32\n",
    "        num_hidden_layers =2\n",
    "        bidirection = True\n",
    "        p_dropout = 0.1     \n",
    "        rnn_class = torch.nn.GRU\n",
    "        nonlin_func = torch.nn.ReLU\n",
    "        pooling_mode = 'none'\n",
    "        l2_reg = 1e-5\n",
    "        batch_size = 1500\n",
    "        num_epochs = 500\n",
    "\n",
    "        model_config_tup = (embed_dim, hidden_dim, z_dim, num_hidden_layers, bidirection, \n",
    "                   p_dropout, rnn_class, nonlin_func, pooling_mode, l2_reg, batch_size, num_epochs)\n",
    "\n",
    "        # input_dim, embed_dim, mlp_embed_factor, nonlin_func, p_dropout, num_encoder_units\n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.ReLU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20\n",
    "\n",
    "        loss_func_name = 'SmoothL1loss'\n",
    "        perfmetric_name = 'pearson'\n",
    "\n",
    "    elif args.model_name == 'Transformer':\n",
    "        embed_dim = 128\n",
    "        num_attn_heads = 4\n",
    "        num_trf_units = 1\n",
    "        pdropout = 0.1\n",
    "        activ_func = torch.nn.GELU\n",
    "        multp_factor = 2\n",
    "        multihead_type = 'Wide'\n",
    "        pos_embed_concat_opt = 'stack'\n",
    "        pooling_opt = 'none'\n",
    "        weight_decay = 1e-8\n",
    "        batch_size = 1000\n",
    "        num_epochs = 1000\n",
    "\n",
    "\n",
    "        model_config_tup = (embed_dim, num_attn_heads, num_trf_units,\n",
    "                            pdropout, activ_func, multp_factor, multihead_type, \n",
    "                            pos_embed_concat_opt, pooling_opt, weight_decay, batch_size, num_epochs)\n",
    "\n",
    "        # input_dim, embed_dim, mlp_embed_factor, nonlin_func, p_dropout, num_encoder_units\n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.GELU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20 \n",
    "\n",
    "        loss_func_name = 'SmoothL1loss'\n",
    "        perfmetric_name = 'pearson'\n",
    "\n",
    "\n",
    "    mconfig, options = build_config_map(args.model_name, \n",
    "                                        optim_tup, \n",
    "                                        model_config_tup, \n",
    "                                        mlpembedder_tup, \n",
    "                                        loss_func = loss_func_name)\n",
    "\n",
    "\n",
    "\n",
    "    options['input_size'] = xproto_inputsize\n",
    "    options['loss_func'] = loss_func_name # to refactor\n",
    "    options['model_name'] = args.model_name\n",
    "    options['perfmetric_name'] = perfmetric_name\n",
    "    return mconfig, options\n",
    "\n",
    "#import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193851f1",
   "metadata": {},
   "source": [
    "### Run training workflow for nerural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a05feff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN\n",
      "--- max normalization ---\n",
      "{'num_epochs': 500, 'weight_decay': 1e-05, 'fdtype': torch.float32, 'to_gpu': True, 'loss_func': 'SmoothL1loss', 'input_size': 20, 'model_name': 'RNN', 'perfmetric_name': 'pearson'}\n",
      "Running model: RNN, exp_name: protospacer, saved at ./output/RNN_v2/protospacer/train_val\n",
      "cuda:0\n",
      "number of runs: 1\n",
      "validation\n",
      "test\n",
      "number of epochs 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay 1e-05\n",
      "Epoch 1/500, Training Loss: 8.0220, Validation Loss: 7.5795\n",
      "Epoch 1/500, best pearson corr. on the validation set so far: 0.0252\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 2/500, Training Loss: 7.4098, Validation Loss: 6.5952\n",
      "Epoch 2/500, best pearson corr. on the validation set so far: 0.0548\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 3/500, Training Loss: 6.2555, Validation Loss: 5.5676\n",
      "Epoch 3/500, best pearson corr. on the validation set so far: 0.1395\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 4/500, Training Loss: 6.0336, Validation Loss: 5.8386\n",
      "Epoch 4/500, best pearson corr. on the validation set so far: 0.2999\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 6/500, Training Loss: 5.8541, Validation Loss: 5.5806\n",
      "Epoch 6/500, best pearson corr. on the validation set so far: 0.3051\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 7/500, Training Loss: 5.7591, Validation Loss: 5.4766\n",
      "Epoch 7/500, best pearson corr. on the validation set so far: 0.3121\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 15/500, Training Loss: 5.4731, Validation Loss: 5.2215\n",
      "Epoch 15/500, best pearson corr. on the validation set so far: 0.3218\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 16/500, Training Loss: 5.3969, Validation Loss: 5.1478\n",
      "Epoch 16/500, best pearson corr. on the validation set so far: 0.3674\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 17/500, Training Loss: 5.3250, Validation Loss: 5.0706\n",
      "Epoch 17/500, best pearson corr. on the validation set so far: 0.4117\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 18/500, Training Loss: 5.2477, Validation Loss: 5.0096\n",
      "Epoch 18/500, best pearson corr. on the validation set so far: 0.4360\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 19/500, Training Loss: 5.1760, Validation Loss: 4.9560\n",
      "Epoch 19/500, best pearson corr. on the validation set so far: 0.4522\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 20/500, Training Loss: 5.1140, Validation Loss: 4.9043\n",
      "Epoch 20/500, best pearson corr. on the validation set so far: 0.4663\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 21/500, Training Loss: 5.0705, Validation Loss: 4.8510\n",
      "Epoch 21/500, best pearson corr. on the validation set so far: 0.4812\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 22/500, Training Loss: 5.0115, Validation Loss: 4.7670\n",
      "Epoch 22/500, best pearson corr. on the validation set so far: 0.5063\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 23/500, Training Loss: 4.9154, Validation Loss: 4.6747\n",
      "Epoch 23/500, best pearson corr. on the validation set so far: 0.5348\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 24/500, Training Loss: 4.8062, Validation Loss: 4.5617\n",
      "Epoch 24/500, best pearson corr. on the validation set so far: 0.5568\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 25/500, Training Loss: 4.7283, Validation Loss: 4.5076\n",
      "Epoch 25/500, best pearson corr. on the validation set so far: 0.5655\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 26/500, Training Loss: 4.6253, Validation Loss: 4.4069\n",
      "Epoch 26/500, best pearson corr. on the validation set so far: 0.5827\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 27/500, Training Loss: 4.5167, Validation Loss: 4.3445\n",
      "Epoch 27/500, best pearson corr. on the validation set so far: 0.5981\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 28/500, Training Loss: 4.4293, Validation Loss: 4.2486\n",
      "Epoch 28/500, best pearson corr. on the validation set so far: 0.6154\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 29/500, Training Loss: 4.3624, Validation Loss: 4.2099\n",
      "Epoch 29/500, best pearson corr. on the validation set so far: 0.6303\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 30/500, Training Loss: 4.3084, Validation Loss: 4.1220\n",
      "Epoch 30/500, best pearson corr. on the validation set so far: 0.6389\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 31/500, Training Loss: 4.2688, Validation Loss: 4.1067\n",
      "Epoch 31/500, best pearson corr. on the validation set so far: 0.6470\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 32/500, Training Loss: 4.2220, Validation Loss: 4.0728\n",
      "Epoch 32/500, best pearson corr. on the validation set so far: 0.6549\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 33/500, Training Loss: 4.1689, Validation Loss: 4.0109\n",
      "Epoch 33/500, best pearson corr. on the validation set so far: 0.6675\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 34/500, Training Loss: 4.0877, Validation Loss: 3.9097\n",
      "Epoch 34/500, best pearson corr. on the validation set so far: 0.6836\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 35/500, Training Loss: 4.0164, Validation Loss: 3.8325\n",
      "Epoch 35/500, best pearson corr. on the validation set so far: 0.6940\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 36/500, Training Loss: 3.9058, Validation Loss: 3.7790\n",
      "Epoch 36/500, best pearson corr. on the validation set so far: 0.7067\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 37/500, Training Loss: 3.8366, Validation Loss: 3.7211\n",
      "Epoch 37/500, best pearson corr. on the validation set so far: 0.7149\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 38/500, Training Loss: 3.7693, Validation Loss: 3.6768\n",
      "Epoch 38/500, best pearson corr. on the validation set so far: 0.7173\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 39/500, Training Loss: 3.7222, Validation Loss: 3.6413\n",
      "Epoch 39/500, best pearson corr. on the validation set so far: 0.7260\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 40/500, Training Loss: 3.6920, Validation Loss: 3.6236\n",
      "Epoch 40/500, best pearson corr. on the validation set so far: 0.7281\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 42/500, Training Loss: 3.6579, Validation Loss: 3.6513\n",
      "Epoch 42/500, best pearson corr. on the validation set so far: 0.7294\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 43/500, Training Loss: 3.6442, Validation Loss: 3.5780\n",
      "Epoch 43/500, best pearson corr. on the validation set so far: 0.7380\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 44/500, Training Loss: 3.6463, Validation Loss: 3.5596\n",
      "Epoch 44/500, best pearson corr. on the validation set so far: 0.7409\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 45/500, Training Loss: 3.5911, Validation Loss: 3.5248\n",
      "Epoch 45/500, best pearson corr. on the validation set so far: 0.7505\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 47/500, Training Loss: 3.5087, Validation Loss: 3.4963\n",
      "Epoch 47/500, best pearson corr. on the validation set so far: 0.7574\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 49/500, Training Loss: 3.4059, Validation Loss: 3.4339\n",
      "Epoch 49/500, best pearson corr. on the validation set so far: 0.7601\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 50/500, Training Loss: 3.3969, Validation Loss: 3.4153\n",
      "Epoch 50/500, best pearson corr. on the validation set so far: 0.7620\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 51/500, Training Loss: 3.3634, Validation Loss: 3.4004\n",
      "Epoch 51/500, best pearson corr. on the validation set so far: 0.7629\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 52/500, Training Loss: 3.3468, Validation Loss: 3.3940\n",
      "Epoch 52/500, best pearson corr. on the validation set so far: 0.7633\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 53/500, Training Loss: 3.3345, Validation Loss: 3.4317\n",
      "Epoch 53/500, best pearson corr. on the validation set so far: 0.7639\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 55/500, Training Loss: 3.3801, Validation Loss: 3.4058\n",
      "Epoch 55/500, best pearson corr. on the validation set so far: 0.7691\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 56/500, Training Loss: 3.2856, Validation Loss: 3.3470\n",
      "Epoch 56/500, best pearson corr. on the validation set so far: 0.7724\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 59/500, Training Loss: 3.1736, Validation Loss: 3.3340\n",
      "Epoch 59/500, best pearson corr. on the validation set so far: 0.7747\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 65/500, Training Loss: 3.0682, Validation Loss: 3.3206\n",
      "Epoch 65/500, best pearson corr. on the validation set so far: 0.7752\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 66/500, Training Loss: 3.0353, Validation Loss: 3.3344\n",
      "Epoch 66/500, best pearson corr. on the validation set so far: 0.7783\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 69/500, Training Loss: 2.9810, Validation Loss: 3.3355\n",
      "Epoch 69/500, best pearson corr. on the validation set so far: 0.7801\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 74/500, Training Loss: 2.9162, Validation Loss: 3.3044\n",
      "Epoch 74/500, best pearson corr. on the validation set so far: 0.7824\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 87/500, Training Loss: 2.7436, Validation Loss: 3.2709\n",
      "Epoch 87/500, best pearson corr. on the validation set so far: 0.7847\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 88/500, Training Loss: 2.6917, Validation Loss: 3.2902\n",
      "Epoch 88/500, best pearson corr. on the validation set so far: 0.7852\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 114/500, Training Loss: 2.2861, Validation Loss: 3.3346\n",
      "Epoch 114/500, best pearson corr. on the validation set so far: 0.7866\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "===============\n",
      "run_name: run_0\n"
     ]
    }
   ],
   "source": [
    "dsettypes = ['train', 'validation','test']\n",
    "gpu_index = 0\n",
    "res_desc = {}\n",
    "version=2\n",
    "for model_name in [ 'RNN']:  #'FFN','CNN', 'RNN',Transformer\n",
    "    print(model_name)\n",
    "    args.model_name =  model_name # {'RNN','CNN', 'Transformer'}\n",
    "    res_desc[model_name] = {}\n",
    "    for exp_name in ['protospacer']: #,'protospacer_extended']:\n",
    "        args.exp_name = exp_name\n",
    "        model_path = os.path.join(args.working_dir, \n",
    "                                  'output', \n",
    "                                  f'{model_name}_v{version}',\n",
    "                                  exp_name)\n",
    "        dpartitions, datatensor_partitions = get_data_ready(args, \n",
    "                                                            normalize_opt='max',\n",
    "                                                            train_size=0.9, \n",
    "                                                            fdtype=torch.float32,\n",
    "                                                            plot_y_distrib=False)\n",
    "        \n",
    "        ## comment out if you want to run for all the data partitions\n",
    "        ## here we only test out for the data partition 0\n",
    "        dpartitions = [dpartitions[0]]\n",
    "        datatensor_partitions = [datatensor_partitions[0]]\n",
    "        \n",
    "        mconfig, options = get_hyperparam_config(args)\n",
    "        print(options)\n",
    "        \n",
    "#         options['num_epochs'] = 10 # use this if you want to test a whole workflow run for all models using 10 epochs\n",
    "        \n",
    "        perfmetric_name = options['perfmetric_name']\n",
    "        train_val_path = os.path.join(model_path, 'train_val')\n",
    "        test_path = os.path.join(model_path, 'test')\n",
    "        \n",
    "        print(f'Running model: {model_name}, exp_name: {exp_name}, saved at {train_val_path}')\n",
    "        perfmetric_run_map, score_run_dict = run_trainevaltest_workflow(datatensor_partitions, \n",
    "                                                                        (mconfig, options), \n",
    "                                                                        train_val_path,\n",
    "                                                                        dsettypes,\n",
    "                                                                        perfmetric_name,\n",
    "                                                                        gpu_index, \n",
    "                                                                        to_gpu=True)\n",
    "        print('='*15)\n",
    "        res_desc[model_name][exp_name] = compute_eval_results_df(train_val_path, len(dpartitions)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef9439a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RNN': {'protospacer':              run_0      mean    median  stddev\n",
       "  spearman  0.765132  0.765132  0.765132     NaN\n",
       "  pearson   0.821761  0.821761  0.821761     NaN\n",
       "  MAE       3.736746  3.736746  3.736746     NaN}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8add254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
