{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a65c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201bb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "from models.data_process import get_datatensor_partitions, prepare_nonproto_features, generate_partition_datatensor,get_data_ready\n",
    "from models.dataset import ProtospacerDataset, ProtospacerExtendedDataset\n",
    "from models.trainval_workflow import run_trainevaltest_workflow\n",
    "from models.trainval_workflow import run_inference\n",
    "from models.hyperparam import build_config_map\n",
    "from src.utils import create_directory, one_hot_encode, get_device, ReaderWriter \n",
    "from src.utils import print_eval_results, plot_y_distrib_acrossfolds, compute_eval_results_df\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f581bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_opt = argparse.ArgumentParser(description='Argparser for data')\n",
    "cmd_opt.add_argument('-model_name',  type=str, help = 'name of the model')\n",
    "cmd_opt.add_argument('-exp_name',  type=str, help = 'name of the experiment')\n",
    "\n",
    "cmd_opt.add_argument('-data_dir',  type=str,default = './data/', help = 'directory of the data')\n",
    "cmd_opt.add_argument('-target_dir',  type=str, default='processed',  help = 'folder name to save the processed data')\n",
    "cmd_opt.add_argument('-working_dir',  type=str, default='./', help = 'the main working directory')\n",
    "cmd_opt.add_argument('-output_path', type=str, help='path to save the trained model')\n",
    "cmd_opt.add_argument('-random_seed', type=int,default=42)\n",
    "cmd_opt.add_argument('-epoch_num', type=int, default =200, help='number of training epochs')\n",
    "args, _ = cmd_opt.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8dbdc3",
   "metadata": {},
   "source": [
    "### Functions to make data ready and get predefined hyperparams for a given model and experiment choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcab8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined hyperparameters depending on the chosen model and experiment\n",
    "def get_hyperparam_config(args):\n",
    "    \"return predefined hyperparameters for each model\"\n",
    "    to_gpu = True\n",
    "    gpu_index=0\n",
    "    optim_tup = None\n",
    "    \n",
    "    if args.model_name == 'FFN':\n",
    "        batch_size = 100\n",
    "        num_epochs = 300\n",
    "        h = [60,10]\n",
    "        l2_reg =0.1\n",
    "        model_config_tup = (h, l2_reg, batch_size, num_epochs)\n",
    "        \n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.ReLU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20\n",
    "        \n",
    "        loss_func_name = 'MSEloss'\n",
    "        perfmetric_name = 'pearson'\n",
    "        \n",
    "    if args.model_name == 'CNN':\n",
    "        k = 2\n",
    "        l2_reg = 0.5\n",
    "        batch_size = 100\n",
    "        num_epochs = 300\n",
    "        model_config_tup = (k, l2_reg, batch_size, num_epochs)\n",
    "\n",
    "\n",
    "        # input_dim, embed_dim, mlp_embed_factor, nonlin_func, p_dropout, num_encoder_units\n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.ReLU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20\n",
    "\n",
    "        loss_func_name = 'MSEloss'\n",
    "        # loss_func_name = 'SmoothL1loss'\n",
    "        perfmetric_name = 'spearman'\n",
    "\n",
    "    elif args.model_name == 'RNN':\n",
    "        embed_dim = 64\n",
    "        hidden_dim = 64\n",
    "        z_dim = 32\n",
    "        num_hidden_layers =2\n",
    "        bidirection = True\n",
    "        p_dropout = 0.1     \n",
    "        rnn_class = torch.nn.GRU\n",
    "        nonlin_func = torch.nn.ReLU\n",
    "        pooling_mode = 'none'\n",
    "        l2_reg = 1e-5\n",
    "        batch_size = 1500\n",
    "        num_epochs = 500\n",
    "\n",
    "        model_config_tup = (embed_dim, hidden_dim, z_dim, num_hidden_layers, bidirection, \n",
    "                   p_dropout, rnn_class, nonlin_func, pooling_mode, l2_reg, batch_size, num_epochs)\n",
    "\n",
    "        # input_dim, embed_dim, mlp_embed_factor, nonlin_func, p_dropout, num_encoder_units\n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.ReLU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20\n",
    "\n",
    "        loss_func_name = 'SmoothL1loss'\n",
    "        perfmetric_name = 'pearson'\n",
    "\n",
    "    elif args.model_name == 'Transformer':\n",
    "        embed_dim = 128\n",
    "        num_attn_heads = 4\n",
    "        num_trf_units = 1\n",
    "        pdropout = 0.1\n",
    "        activ_func = torch.nn.GELU\n",
    "        multp_factor = 2\n",
    "        multihead_type = 'Wide'\n",
    "        pos_embed_concat_opt = 'stack'\n",
    "        pooling_opt = 'none'\n",
    "        weight_decay = 1e-8\n",
    "        batch_size = 1000\n",
    "        num_epochs = 1000\n",
    "\n",
    "\n",
    "        model_config_tup = (embed_dim, num_attn_heads, num_trf_units,\n",
    "                            pdropout, activ_func, multp_factor, multihead_type, \n",
    "                            pos_embed_concat_opt, pooling_opt, weight_decay, batch_size, num_epochs)\n",
    "\n",
    "        # input_dim, embed_dim, mlp_embed_factor, nonlin_func, p_dropout, num_encoder_units\n",
    "        if args.exp_name == 'protospacer_extended':\n",
    "            mlpembedder_tup = (10, 16, 2, torch.nn.GELU, 0.1, 1)\n",
    "            xproto_inputsize = 20 + 10\n",
    "        else:\n",
    "            mlpembedder_tup = None\n",
    "            xproto_inputsize = 20 \n",
    "\n",
    "        loss_func_name = 'SmoothL1loss'\n",
    "        perfmetric_name = 'pearson'\n",
    "\n",
    "\n",
    "    mconfig, options = build_config_map(args.model_name, \n",
    "                                        optim_tup, \n",
    "                                        model_config_tup, \n",
    "                                        mlpembedder_tup, \n",
    "                                        loss_func = loss_func_name)\n",
    "\n",
    "\n",
    "\n",
    "    options['input_size'] = xproto_inputsize\n",
    "    options['loss_func'] = loss_func_name # to refactor\n",
    "    options['model_name'] = args.model_name\n",
    "    options['perfmetric_name'] = perfmetric_name\n",
    "    return mconfig, options\n",
    "\n",
    "#import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193851f1",
   "metadata": {},
   "source": [
    "### Run training workflow for nerural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a05feff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN\n",
      "--- max normalization ---\n",
      "{'run_num': -1, 'num_epochs': 500, 'weight_decay': 1e-05, 'fdtype': torch.float32, 'to_gpu': True, 'loss_func': 'SmoothL1loss', 'input_size': 20, 'model_name': 'RNN', 'perfmetric_name': 'pearson'}\n",
      "Running model: RNN, exp_name: protospacer, saved at ./output/RNN_v2/protospacer/train_val\n",
      "cuda:0\n",
      "validation\n",
      "test\n",
      "number of epochs 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/bedict_crispr/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay 1e-05\n",
      "Epoch 1/500, Training Loss: 8.0201, Validation Loss: 7.5960\n",
      "Epoch 1/500, best pearson corr. on the validation set so far: 0.1687\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 2/500, Training Loss: 7.4207, Validation Loss: 6.5753\n",
      "Epoch 2/500, best pearson corr. on the validation set so far: 0.2184\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 3/500, Training Loss: 6.2234, Validation Loss: 5.5518\n",
      "Epoch 3/500, best pearson corr. on the validation set so far: 0.2338\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 4/500, Training Loss: 6.0507, Validation Loss: 5.8290\n",
      "Epoch 4/500, best pearson corr. on the validation set so far: 0.2567\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 12/500, Training Loss: 5.5505, Validation Loss: 5.3204\n",
      "Epoch 12/500, best pearson corr. on the validation set so far: 0.2614\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 13/500, Training Loss: 5.5273, Validation Loss: 5.2819\n",
      "Epoch 13/500, best pearson corr. on the validation set so far: 0.2796\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 14/500, Training Loss: 5.4914, Validation Loss: 5.2359\n",
      "Epoch 14/500, best pearson corr. on the validation set so far: 0.3076\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 15/500, Training Loss: 5.4309, Validation Loss: 5.1801\n",
      "Epoch 15/500, best pearson corr. on the validation set so far: 0.3526\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 16/500, Training Loss: 5.3428, Validation Loss: 5.0666\n",
      "Epoch 16/500, best pearson corr. on the validation set so far: 0.4077\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 17/500, Training Loss: 5.2213, Validation Loss: 4.9569\n",
      "Epoch 17/500, best pearson corr. on the validation set so far: 0.4549\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 18/500, Training Loss: 5.0984, Validation Loss: 4.8201\n",
      "Epoch 18/500, best pearson corr. on the validation set so far: 0.4961\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 19/500, Training Loss: 4.9868, Validation Loss: 4.7561\n",
      "Epoch 19/500, best pearson corr. on the validation set so far: 0.5269\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 20/500, Training Loss: 4.9062, Validation Loss: 4.6844\n",
      "Epoch 20/500, best pearson corr. on the validation set so far: 0.5425\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 21/500, Training Loss: 4.8513, Validation Loss: 4.6446\n",
      "Epoch 21/500, best pearson corr. on the validation set so far: 0.5518\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 22/500, Training Loss: 4.7904, Validation Loss: 4.5649\n",
      "Epoch 22/500, best pearson corr. on the validation set so far: 0.5663\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 23/500, Training Loss: 4.7134, Validation Loss: 4.4889\n",
      "Epoch 23/500, best pearson corr. on the validation set so far: 0.5825\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 24/500, Training Loss: 4.6263, Validation Loss: 4.3943\n",
      "Epoch 24/500, best pearson corr. on the validation set so far: 0.6003\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 25/500, Training Loss: 4.5214, Validation Loss: 4.2976\n",
      "Epoch 25/500, best pearson corr. on the validation set so far: 0.6255\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 26/500, Training Loss: 4.4212, Validation Loss: 4.2203\n",
      "Epoch 26/500, best pearson corr. on the validation set so far: 0.6404\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 27/500, Training Loss: 4.3515, Validation Loss: 4.1565\n",
      "Epoch 27/500, best pearson corr. on the validation set so far: 0.6484\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 28/500, Training Loss: 4.2828, Validation Loss: 4.1588\n",
      "Epoch 28/500, best pearson corr. on the validation set so far: 0.6554\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 29/500, Training Loss: 4.2281, Validation Loss: 4.0515\n",
      "Epoch 29/500, best pearson corr. on the validation set so far: 0.6637\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 31/500, Training Loss: 4.1555, Validation Loss: 4.0252\n",
      "Epoch 31/500, best pearson corr. on the validation set so far: 0.6664\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 32/500, Training Loss: 4.1257, Validation Loss: 4.0061\n",
      "Epoch 32/500, best pearson corr. on the validation set so far: 0.6693\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 33/500, Training Loss: 4.0834, Validation Loss: 3.9648\n",
      "Epoch 33/500, best pearson corr. on the validation set so far: 0.6782\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 34/500, Training Loss: 4.0500, Validation Loss: 3.9165\n",
      "Epoch 34/500, best pearson corr. on the validation set so far: 0.6831\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 35/500, Training Loss: 3.9867, Validation Loss: 3.8805\n",
      "Epoch 35/500, best pearson corr. on the validation set so far: 0.6840\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 36/500, Training Loss: 3.9248, Validation Loss: 3.8486\n",
      "Epoch 36/500, best pearson corr. on the validation set so far: 0.6941\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 37/500, Training Loss: 3.8446, Validation Loss: 3.7935\n",
      "Epoch 37/500, best pearson corr. on the validation set so far: 0.6988\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 38/500, Training Loss: 3.7899, Validation Loss: 3.7627\n",
      "Epoch 38/500, best pearson corr. on the validation set so far: 0.7067\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 39/500, Training Loss: 3.7461, Validation Loss: 3.7339\n",
      "Epoch 39/500, best pearson corr. on the validation set so far: 0.7084\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 40/500, Training Loss: 3.7214, Validation Loss: 3.6987\n",
      "Epoch 40/500, best pearson corr. on the validation set so far: 0.7133\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 41/500, Training Loss: 3.7016, Validation Loss: 3.6878\n",
      "Epoch 41/500, best pearson corr. on the validation set so far: 0.7163\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 42/500, Training Loss: 3.6727, Validation Loss: 3.6420\n",
      "Epoch 42/500, best pearson corr. on the validation set so far: 0.7213\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 43/500, Training Loss: 3.6502, Validation Loss: 3.6636\n",
      "Epoch 43/500, best pearson corr. on the validation set so far: 0.7236\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 44/500, Training Loss: 3.6139, Validation Loss: 3.5986\n",
      "Epoch 44/500, best pearson corr. on the validation set so far: 0.7332\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 45/500, Training Loss: 3.5535, Validation Loss: 3.5374\n",
      "Epoch 45/500, best pearson corr. on the validation set so far: 0.7416\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 46/500, Training Loss: 3.5460, Validation Loss: 3.5460\n",
      "Epoch 46/500, best pearson corr. on the validation set so far: 0.7422\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 47/500, Training Loss: 3.4889, Validation Loss: 3.5113\n",
      "Epoch 47/500, best pearson corr. on the validation set so far: 0.7575\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 48/500, Training Loss: 3.4837, Validation Loss: 3.5296\n",
      "Epoch 48/500, best pearson corr. on the validation set so far: 0.7579\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 51/500, Training Loss: 3.3606, Validation Loss: 3.4407\n",
      "Epoch 51/500, best pearson corr. on the validation set so far: 0.7612\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 54/500, Training Loss: 3.3045, Validation Loss: 3.4063\n",
      "Epoch 54/500, best pearson corr. on the validation set so far: 0.7667\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 55/500, Training Loss: 3.2622, Validation Loss: 3.3874\n",
      "Epoch 55/500, best pearson corr. on the validation set so far: 0.7680\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 56/500, Training Loss: 3.2648, Validation Loss: 3.3919\n",
      "Epoch 56/500, best pearson corr. on the validation set so far: 0.7701\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 57/500, Training Loss: 3.2068, Validation Loss: 3.3761\n",
      "Epoch 57/500, best pearson corr. on the validation set so far: 0.7725\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 58/500, Training Loss: 3.1713, Validation Loss: 3.3595\n",
      "Epoch 58/500, best pearson corr. on the validation set so far: 0.7737\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 61/500, Training Loss: 3.1246, Validation Loss: 3.3550\n",
      "Epoch 61/500, best pearson corr. on the validation set so far: 0.7748\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 62/500, Training Loss: 3.1275, Validation Loss: 3.3524\n",
      "Epoch 62/500, best pearson corr. on the validation set so far: 0.7770\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 64/500, Training Loss: 3.0968, Validation Loss: 3.3370\n",
      "Epoch 64/500, best pearson corr. on the validation set so far: 0.7779\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 65/500, Training Loss: 3.0639, Validation Loss: 3.4414\n",
      "Epoch 65/500, best pearson corr. on the validation set so far: 0.7804\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 66/500, Training Loss: 3.0504, Validation Loss: 3.3131\n",
      "Epoch 66/500, best pearson corr. on the validation set so far: 0.7828\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 72/500, Training Loss: 2.9103, Validation Loss: 3.3177\n",
      "Epoch 72/500, best pearson corr. on the validation set so far: 0.7832\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 75/500, Training Loss: 2.8981, Validation Loss: 3.3236\n",
      "Epoch 75/500, best pearson corr. on the validation set so far: 0.7834\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500, Training Loss: 2.7911, Validation Loss: 3.3178\n",
      "Epoch 79/500, best pearson corr. on the validation set so far: 0.7846\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "validation\n",
      "test\n",
      "number of epochs 500\n",
      "weight_decay 1e-05\n",
      "Epoch 2/500, Training Loss: 7.5046, Validation Loss: 7.0090\n",
      "Epoch 2/500, best pearson corr. on the validation set so far: 0.0095\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 3/500, Training Loss: 6.3837, Validation Loss: 5.7730\n",
      "Epoch 3/500, best pearson corr. on the validation set so far: 0.1059\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 4/500, Training Loss: 5.9360, Validation Loss: 6.0606\n",
      "Epoch 4/500, best pearson corr. on the validation set so far: 0.2710\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 5/500, Training Loss: 5.8899, Validation Loss: 5.7572\n",
      "Epoch 5/500, best pearson corr. on the validation set so far: 0.3335\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 6/500, Training Loss: 5.8108, Validation Loss: 5.8366\n",
      "Epoch 6/500, best pearson corr. on the validation set so far: 0.3348\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 7/500, Training Loss: 5.7528, Validation Loss: 5.6952\n",
      "Epoch 7/500, best pearson corr. on the validation set so far: 0.3401\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 15/500, Training Loss: 5.4280, Validation Loss: 5.3589\n",
      "Epoch 15/500, best pearson corr. on the validation set so far: 0.3712\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 16/500, Training Loss: 5.3431, Validation Loss: 5.2465\n",
      "Epoch 16/500, best pearson corr. on the validation set so far: 0.4200\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 17/500, Training Loss: 5.2370, Validation Loss: 5.1414\n",
      "Epoch 17/500, best pearson corr. on the validation set so far: 0.4606\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 18/500, Training Loss: 5.1547, Validation Loss: 5.0620\n",
      "Epoch 18/500, best pearson corr. on the validation set so far: 0.4862\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 19/500, Training Loss: 5.0674, Validation Loss: 4.9857\n",
      "Epoch 19/500, best pearson corr. on the validation set so far: 0.5019\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 20/500, Training Loss: 5.0000, Validation Loss: 4.9392\n",
      "Epoch 20/500, best pearson corr. on the validation set so far: 0.5125\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 21/500, Training Loss: 4.9469, Validation Loss: 4.8984\n",
      "Epoch 21/500, best pearson corr. on the validation set so far: 0.5201\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 22/500, Training Loss: 4.8782, Validation Loss: 4.8273\n",
      "Epoch 22/500, best pearson corr. on the validation set so far: 0.5307\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 23/500, Training Loss: 4.7767, Validation Loss: 4.7576\n",
      "Epoch 23/500, best pearson corr. on the validation set so far: 0.5459\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 24/500, Training Loss: 4.6906, Validation Loss: 4.6314\n",
      "Epoch 24/500, best pearson corr. on the validation set so far: 0.5652\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 25/500, Training Loss: 4.5734, Validation Loss: 4.5223\n",
      "Epoch 25/500, best pearson corr. on the validation set so far: 0.5930\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 26/500, Training Loss: 4.4301, Validation Loss: 4.3671\n",
      "Epoch 26/500, best pearson corr. on the validation set so far: 0.6219\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 27/500, Training Loss: 4.2972, Validation Loss: 4.2659\n",
      "Epoch 27/500, best pearson corr. on the validation set so far: 0.6373\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 28/500, Training Loss: 4.1940, Validation Loss: 4.1958\n",
      "Epoch 28/500, best pearson corr. on the validation set so far: 0.6536\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 29/500, Training Loss: 4.1145, Validation Loss: 4.1464\n",
      "Epoch 29/500, best pearson corr. on the validation set so far: 0.6643\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 30/500, Training Loss: 4.0630, Validation Loss: 4.1333\n",
      "Epoch 30/500, best pearson corr. on the validation set so far: 0.6671\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 31/500, Training Loss: 4.0302, Validation Loss: 4.1061\n",
      "Epoch 31/500, best pearson corr. on the validation set so far: 0.6714\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 32/500, Training Loss: 3.9880, Validation Loss: 4.0649\n",
      "Epoch 32/500, best pearson corr. on the validation set so far: 0.6804\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 33/500, Training Loss: 3.9569, Validation Loss: 4.0137\n",
      "Epoch 33/500, best pearson corr. on the validation set so far: 0.6902\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 34/500, Training Loss: 3.8840, Validation Loss: 3.9555\n",
      "Epoch 34/500, best pearson corr. on the validation set so far: 0.6998\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 35/500, Training Loss: 3.7972, Validation Loss: 3.8717\n",
      "Epoch 35/500, best pearson corr. on the validation set so far: 0.7135\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 36/500, Training Loss: 3.7250, Validation Loss: 3.8103\n",
      "Epoch 36/500, best pearson corr. on the validation set so far: 0.7222\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 37/500, Training Loss: 3.6564, Validation Loss: 3.8560\n",
      "Epoch 37/500, best pearson corr. on the validation set so far: 0.7321\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 38/500, Training Loss: 3.6290, Validation Loss: 3.7138\n",
      "Epoch 38/500, best pearson corr. on the validation set so far: 0.7393\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 39/500, Training Loss: 3.5561, Validation Loss: 3.6848\n",
      "Epoch 39/500, best pearson corr. on the validation set so far: 0.7429\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 40/500, Training Loss: 3.5168, Validation Loss: 3.6588\n",
      "Epoch 40/500, best pearson corr. on the validation set so far: 0.7466\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 41/500, Training Loss: 3.4946, Validation Loss: 3.6366\n",
      "Epoch 41/500, best pearson corr. on the validation set so far: 0.7473\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 42/500, Training Loss: 3.4825, Validation Loss: 3.6096\n",
      "Epoch 42/500, best pearson corr. on the validation set so far: 0.7504\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 43/500, Training Loss: 3.4477, Validation Loss: 3.5947\n",
      "Epoch 43/500, best pearson corr. on the validation set so far: 0.7530\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 44/500, Training Loss: 3.4188, Validation Loss: 3.5596\n",
      "Epoch 44/500, best pearson corr. on the validation set so far: 0.7572\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 45/500, Training Loss: 3.4432, Validation Loss: 3.5196\n",
      "Epoch 45/500, best pearson corr. on the validation set so far: 0.7655\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 46/500, Training Loss: 3.3685, Validation Loss: 3.5085\n",
      "Epoch 46/500, best pearson corr. on the validation set so far: 0.7671\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 47/500, Training Loss: 3.3020, Validation Loss: 3.4936\n",
      "Epoch 47/500, best pearson corr. on the validation set so far: 0.7703\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 49/500, Training Loss: 3.2397, Validation Loss: 3.4910\n",
      "Epoch 49/500, best pearson corr. on the validation set so far: 0.7718\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 50/500, Training Loss: 3.1993, Validation Loss: 3.4573\n",
      "Epoch 50/500, best pearson corr. on the validation set so far: 0.7738\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 52/500, Training Loss: 3.1766, Validation Loss: 3.4354\n",
      "Epoch 52/500, best pearson corr. on the validation set so far: 0.7767\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 53/500, Training Loss: 3.1589, Validation Loss: 3.4207\n",
      "Epoch 53/500, best pearson corr. on the validation set so far: 0.7787\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 54/500, Training Loss: 3.1513, Validation Loss: 3.4270\n",
      "Epoch 54/500, best pearson corr. on the validation set so far: 0.7796\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 56/500, Training Loss: 3.0795, Validation Loss: 3.3819\n",
      "Epoch 56/500, best pearson corr. on the validation set so far: 0.7839\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 58/500, Training Loss: 3.0392, Validation Loss: 3.3587\n",
      "Epoch 58/500, best pearson corr. on the validation set so far: 0.7864\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 59/500, Training Loss: 2.9985, Validation Loss: 3.3548\n",
      "Epoch 59/500, best pearson corr. on the validation set so far: 0.7864\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 61/500, Training Loss: 2.9777, Validation Loss: 3.3637\n",
      "Epoch 61/500, best pearson corr. on the validation set so far: 0.7885\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 63/500, Training Loss: 2.9707, Validation Loss: 3.5209\n",
      "Epoch 63/500, best pearson corr. on the validation set so far: 0.7905\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 65/500, Training Loss: 2.9726, Validation Loss: 3.3983\n",
      "Epoch 65/500, best pearson corr. on the validation set so far: 0.7920\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 69/500, Training Loss: 2.8368, Validation Loss: 3.3120\n",
      "Epoch 69/500, best pearson corr. on the validation set so far: 0.7927\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/500, Training Loss: 2.8241, Validation Loss: 3.3187\n",
      "Epoch 70/500, best pearson corr. on the validation set so far: 0.7960\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 81/500, Training Loss: 2.6774, Validation Loss: 3.3036\n",
      "Epoch 81/500, best pearson corr. on the validation set so far: 0.7962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 85/500, Training Loss: 2.7785, Validation Loss: 3.2852\n",
      "Epoch 85/500, best pearson corr. on the validation set so far: 0.7974\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 86/500, Training Loss: 2.6853, Validation Loss: 3.4331\n",
      "Epoch 86/500, best pearson corr. on the validation set so far: 0.7995\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "validation\n",
      "test\n",
      "number of epochs 500\n",
      "weight_decay 1e-05\n",
      "Epoch 1/500, Training Loss: 8.0868, Validation Loss: 7.8358\n",
      "Epoch 1/500, best pearson corr. on the validation set so far: 0.0490\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 2/500, Training Loss: 7.6941, Validation Loss: 7.1889\n",
      "Epoch 2/500, best pearson corr. on the validation set so far: 0.0828\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 3/500, Training Loss: 6.7455, Validation Loss: 5.8816\n",
      "Epoch 3/500, best pearson corr. on the validation set so far: 0.1128\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 4/500, Training Loss: 5.9473, Validation Loss: 6.1627\n",
      "Epoch 4/500, best pearson corr. on the validation set so far: 0.2625\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 5/500, Training Loss: 6.0427, Validation Loss: 5.7249\n",
      "Epoch 5/500, best pearson corr. on the validation set so far: 0.3493\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 6/500, Training Loss: 5.8527, Validation Loss: 5.8179\n",
      "Epoch 6/500, best pearson corr. on the validation set so far: 0.3607\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 7/500, Training Loss: 5.8413, Validation Loss: 5.7046\n",
      "Epoch 7/500, best pearson corr. on the validation set so far: 0.3649\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 8/500, Training Loss: 5.7747, Validation Loss: 5.7001\n",
      "Epoch 8/500, best pearson corr. on the validation set so far: 0.3667\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 16/500, Training Loss: 5.4634, Validation Loss: 5.3623\n",
      "Epoch 16/500, best pearson corr. on the validation set so far: 0.3751\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 17/500, Training Loss: 5.3822, Validation Loss: 5.2711\n",
      "Epoch 17/500, best pearson corr. on the validation set so far: 0.4257\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 18/500, Training Loss: 5.3150, Validation Loss: 5.2144\n",
      "Epoch 18/500, best pearson corr. on the validation set so far: 0.4596\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 19/500, Training Loss: 5.2441, Validation Loss: 5.1137\n",
      "Epoch 19/500, best pearson corr. on the validation set so far: 0.4779\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 20/500, Training Loss: 5.1840, Validation Loss: 5.0710\n",
      "Epoch 20/500, best pearson corr. on the validation set so far: 0.4884\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 21/500, Training Loss: 5.1287, Validation Loss: 5.0196\n",
      "Epoch 21/500, best pearson corr. on the validation set so far: 0.5014\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 22/500, Training Loss: 5.0759, Validation Loss: 4.9380\n",
      "Epoch 22/500, best pearson corr. on the validation set so far: 0.5233\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 23/500, Training Loss: 4.9765, Validation Loss: 4.8130\n",
      "Epoch 23/500, best pearson corr. on the validation set so far: 0.5486\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 24/500, Training Loss: 4.8596, Validation Loss: 4.7299\n",
      "Epoch 24/500, best pearson corr. on the validation set so far: 0.5642\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 25/500, Training Loss: 4.7347, Validation Loss: 4.6617\n",
      "Epoch 25/500, best pearson corr. on the validation set so far: 0.5790\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 26/500, Training Loss: 4.6307, Validation Loss: 4.5483\n",
      "Epoch 26/500, best pearson corr. on the validation set so far: 0.5985\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 27/500, Training Loss: 4.5184, Validation Loss: 4.4194\n",
      "Epoch 27/500, best pearson corr. on the validation set so far: 0.6159\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 28/500, Training Loss: 4.4151, Validation Loss: 4.3417\n",
      "Epoch 28/500, best pearson corr. on the validation set so far: 0.6357\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 29/500, Training Loss: 4.3400, Validation Loss: 4.2808\n",
      "Epoch 29/500, best pearson corr. on the validation set so far: 0.6455\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 30/500, Training Loss: 4.2908, Validation Loss: 4.2434\n",
      "Epoch 30/500, best pearson corr. on the validation set so far: 0.6492\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 31/500, Training Loss: 4.2602, Validation Loss: 4.2736\n",
      "Epoch 31/500, best pearson corr. on the validation set so far: 0.6533\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 32/500, Training Loss: 4.2388, Validation Loss: 4.1943\n",
      "Epoch 32/500, best pearson corr. on the validation set so far: 0.6615\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 33/500, Training Loss: 4.1779, Validation Loss: 4.1488\n",
      "Epoch 33/500, best pearson corr. on the validation set so far: 0.6658\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 34/500, Training Loss: 4.1271, Validation Loss: 4.1497\n",
      "Epoch 34/500, best pearson corr. on the validation set so far: 0.6726\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 35/500, Training Loss: 4.0350, Validation Loss: 4.0728\n",
      "Epoch 35/500, best pearson corr. on the validation set so far: 0.6819\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 36/500, Training Loss: 3.9642, Validation Loss: 4.0148\n",
      "Epoch 36/500, best pearson corr. on the validation set so far: 0.6858\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 37/500, Training Loss: 3.8921, Validation Loss: 3.9760\n",
      "Epoch 37/500, best pearson corr. on the validation set so far: 0.6927\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 38/500, Training Loss: 3.8546, Validation Loss: 3.9528\n",
      "Epoch 38/500, best pearson corr. on the validation set so far: 0.6951\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 39/500, Training Loss: 3.8129, Validation Loss: 3.9577\n",
      "Epoch 39/500, best pearson corr. on the validation set so far: 0.6998\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 40/500, Training Loss: 3.7978, Validation Loss: 3.9144\n",
      "Epoch 40/500, best pearson corr. on the validation set so far: 0.7006\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 42/500, Training Loss: 3.7498, Validation Loss: 3.9046\n",
      "Epoch 42/500, best pearson corr. on the validation set so far: 0.7057\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 43/500, Training Loss: 3.7186, Validation Loss: 3.8562\n",
      "Epoch 43/500, best pearson corr. on the validation set so far: 0.7083\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 44/500, Training Loss: 3.6650, Validation Loss: 3.8416\n",
      "Epoch 44/500, best pearson corr. on the validation set so far: 0.7165\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 45/500, Training Loss: 3.6373, Validation Loss: 3.7430\n",
      "Epoch 45/500, best pearson corr. on the validation set so far: 0.7221\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 46/500, Training Loss: 3.5523, Validation Loss: 3.7094\n",
      "Epoch 46/500, best pearson corr. on the validation set so far: 0.7330\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 48/500, Training Loss: 3.4843, Validation Loss: 3.6776\n",
      "Epoch 48/500, best pearson corr. on the validation set so far: 0.7406\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 50/500, Training Loss: 3.4144, Validation Loss: 3.6316\n",
      "Epoch 50/500, best pearson corr. on the validation set so far: 0.7437\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 51/500, Training Loss: 3.4115, Validation Loss: 3.6028\n",
      "Epoch 51/500, best pearson corr. on the validation set so far: 0.7445\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 52/500, Training Loss: 3.3939, Validation Loss: 3.6054\n",
      "Epoch 52/500, best pearson corr. on the validation set so far: 0.7468\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 53/500, Training Loss: 3.3946, Validation Loss: 3.6472\n",
      "Epoch 53/500, best pearson corr. on the validation set so far: 0.7511\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 54/500, Training Loss: 3.4218, Validation Loss: 3.5504\n",
      "Epoch 54/500, best pearson corr. on the validation set so far: 0.7530\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 55/500, Training Loss: 3.3612, Validation Loss: 3.5416\n",
      "Epoch 55/500, best pearson corr. on the validation set so far: 0.7595\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 56/500, Training Loss: 3.3358, Validation Loss: 3.5052\n",
      "Epoch 56/500, best pearson corr. on the validation set so far: 0.7676\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 58/500, Training Loss: 3.2420, Validation Loss: 3.4566\n",
      "Epoch 58/500, best pearson corr. on the validation set so far: 0.7677\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 59/500, Training Loss: 3.2303, Validation Loss: 3.4693\n",
      "Epoch 59/500, best pearson corr. on the validation set so far: 0.7736\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/500, Training Loss: 3.1189, Validation Loss: 3.4079\n",
      "Epoch 64/500, best pearson corr. on the validation set so far: 0.7795\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 66/500, Training Loss: 3.0967, Validation Loss: 3.3836\n",
      "Epoch 66/500, best pearson corr. on the validation set so far: 0.7809\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 67/500, Training Loss: 3.0877, Validation Loss: 3.3554\n",
      "Epoch 67/500, best pearson corr. on the validation set so far: 0.7848\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 69/500, Training Loss: 2.9996, Validation Loss: 3.3420\n",
      "Epoch 69/500, best pearson corr. on the validation set so far: 0.7866\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 72/500, Training Loss: 2.9693, Validation Loss: 3.3227\n",
      "Epoch 72/500, best pearson corr. on the validation set so far: 0.7890\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 74/500, Training Loss: 2.9547, Validation Loss: 3.2946\n",
      "Epoch 74/500, best pearson corr. on the validation set so far: 0.7912\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 75/500, Training Loss: 2.9278, Validation Loss: 3.3356\n",
      "Epoch 75/500, best pearson corr. on the validation set so far: 0.7919\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 76/500, Training Loss: 2.9324, Validation Loss: 3.2708\n",
      "Epoch 76/500, best pearson corr. on the validation set so far: 0.7969\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 78/500, Training Loss: 2.8606, Validation Loss: 3.2643\n",
      "Epoch 78/500, best pearson corr. on the validation set so far: 0.7977\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 81/500, Training Loss: 2.7978, Validation Loss: 3.2802\n",
      "Epoch 81/500, best pearson corr. on the validation set so far: 0.7986\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 83/500, Training Loss: 2.7727, Validation Loss: 3.2457\n",
      "Epoch 83/500, best pearson corr. on the validation set so far: 0.8001\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 84/500, Training Loss: 2.7667, Validation Loss: 3.2315\n",
      "Epoch 84/500, best pearson corr. on the validation set so far: 0.8024\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 88/500, Training Loss: 2.6953, Validation Loss: 3.2251\n",
      "Epoch 88/500, best pearson corr. on the validation set so far: 0.8026\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 91/500, Training Loss: 2.6299, Validation Loss: 3.2234\n",
      "Epoch 91/500, best pearson corr. on the validation set so far: 0.8041\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 97/500, Training Loss: 2.5535, Validation Loss: 3.1726\n",
      "Epoch 97/500, best pearson corr. on the validation set so far: 0.8078\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 99/500, Training Loss: 2.4930, Validation Loss: 3.1687\n",
      "Epoch 99/500, best pearson corr. on the validation set so far: 0.8081\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 104/500, Training Loss: 2.5295, Validation Loss: 3.1624\n",
      "Epoch 104/500, best pearson corr. on the validation set so far: 0.8096\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 107/500, Training Loss: 2.4748, Validation Loss: 3.1835\n",
      "Epoch 107/500, best pearson corr. on the validation set so far: 0.8118\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 147/500, Training Loss: 1.9520, Validation Loss: 3.1691\n",
      "Epoch 147/500, best pearson corr. on the validation set so far: 0.8121\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 166/500, Training Loss: 1.7539, Validation Loss: 3.2020\n",
      "Epoch 166/500, best pearson corr. on the validation set so far: 0.8130\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "validation\n",
      "test\n",
      "number of epochs 500\n",
      "weight_decay 1e-05\n",
      "Epoch 1/500, Training Loss: 7.8808, Validation Loss: 8.0432\n",
      "Epoch 1/500, best pearson corr. on the validation set so far: 0.1817\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 2/500, Training Loss: 7.2766, Validation Loss: 7.0421\n",
      "Epoch 2/500, best pearson corr. on the validation set so far: 0.2460\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 3/500, Training Loss: 6.0364, Validation Loss: 6.1097\n",
      "Epoch 3/500, best pearson corr. on the validation set so far: 0.2944\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 4/500, Training Loss: 5.9769, Validation Loss: 6.2728\n",
      "Epoch 4/500, best pearson corr. on the validation set so far: 0.3240\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 5/500, Training Loss: 5.7284, Validation Loss: 6.1452\n",
      "Epoch 5/500, best pearson corr. on the validation set so far: 0.3274\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 6/500, Training Loss: 5.7660, Validation Loss: 6.1314\n",
      "Epoch 6/500, best pearson corr. on the validation set so far: 0.3333\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 7/500, Training Loss: 5.6101, Validation Loss: 5.9721\n",
      "Epoch 7/500, best pearson corr. on the validation set so far: 0.3388\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 11/500, Training Loss: 5.4873, Validation Loss: 5.8494\n",
      "Epoch 11/500, best pearson corr. on the validation set so far: 0.3406\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 12/500, Training Loss: 5.4626, Validation Loss: 5.8232\n",
      "Epoch 12/500, best pearson corr. on the validation set so far: 0.3460\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 13/500, Training Loss: 5.4489, Validation Loss: 5.7944\n",
      "Epoch 13/500, best pearson corr. on the validation set so far: 0.3566\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 14/500, Training Loss: 5.4177, Validation Loss: 5.7602\n",
      "Epoch 14/500, best pearson corr. on the validation set so far: 0.3723\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 15/500, Training Loss: 5.3842, Validation Loss: 5.7072\n",
      "Epoch 15/500, best pearson corr. on the validation set so far: 0.3975\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 16/500, Training Loss: 5.3308, Validation Loss: 5.6465\n",
      "Epoch 16/500, best pearson corr. on the validation set so far: 0.4298\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 17/500, Training Loss: 5.2678, Validation Loss: 5.5618\n",
      "Epoch 17/500, best pearson corr. on the validation set so far: 0.4579\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 18/500, Training Loss: 5.2001, Validation Loss: 5.4737\n",
      "Epoch 18/500, best pearson corr. on the validation set so far: 0.4797\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 19/500, Training Loss: 5.1390, Validation Loss: 5.4029\n",
      "Epoch 19/500, best pearson corr. on the validation set so far: 0.4981\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 20/500, Training Loss: 5.0799, Validation Loss: 5.3528\n",
      "Epoch 20/500, best pearson corr. on the validation set so far: 0.5118\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 21/500, Training Loss: 5.0424, Validation Loss: 5.3031\n",
      "Epoch 21/500, best pearson corr. on the validation set so far: 0.5229\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 22/500, Training Loss: 4.9864, Validation Loss: 5.2236\n",
      "Epoch 22/500, best pearson corr. on the validation set so far: 0.5391\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 23/500, Training Loss: 4.9017, Validation Loss: 5.1299\n",
      "Epoch 23/500, best pearson corr. on the validation set so far: 0.5569\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 24/500, Training Loss: 4.7899, Validation Loss: 5.0144\n",
      "Epoch 24/500, best pearson corr. on the validation set so far: 0.5745\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 25/500, Training Loss: 4.6578, Validation Loss: 4.9066\n",
      "Epoch 25/500, best pearson corr. on the validation set so far: 0.5927\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 26/500, Training Loss: 4.5276, Validation Loss: 4.8082\n",
      "Epoch 26/500, best pearson corr. on the validation set so far: 0.6111\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 27/500, Training Loss: 4.4051, Validation Loss: 4.6586\n",
      "Epoch 27/500, best pearson corr. on the validation set so far: 0.6385\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 28/500, Training Loss: 4.2872, Validation Loss: 4.5631\n",
      "Epoch 28/500, best pearson corr. on the validation set so far: 0.6595\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 29/500, Training Loss: 4.1999, Validation Loss: 4.5191\n",
      "Epoch 29/500, best pearson corr. on the validation set so far: 0.6706\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 30/500, Training Loss: 4.1576, Validation Loss: 4.4347\n",
      "Epoch 30/500, best pearson corr. on the validation set so far: 0.6806\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 31/500, Training Loss: 4.1010, Validation Loss: 4.4130\n",
      "Epoch 31/500, best pearson corr. on the validation set so far: 0.6826\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 32/500, Training Loss: 4.0665, Validation Loss: 4.3261\n",
      "Epoch 32/500, best pearson corr. on the validation set so far: 0.6932\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 33/500, Training Loss: 4.0091, Validation Loss: 4.2681\n",
      "Epoch 33/500, best pearson corr. on the validation set so far: 0.7078\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 34/500, Training Loss: 3.9094, Validation Loss: 4.2081\n",
      "Epoch 34/500, best pearson corr. on the validation set so far: 0.7181\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 35/500, Training Loss: 3.8221, Validation Loss: 4.1056\n",
      "Epoch 35/500, best pearson corr. on the validation set so far: 0.7251\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500, Training Loss: 3.7662, Validation Loss: 4.1761\n",
      "Epoch 36/500, best pearson corr. on the validation set so far: 0.7329\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 37/500, Training Loss: 3.7259, Validation Loss: 4.0290\n",
      "Epoch 37/500, best pearson corr. on the validation set so far: 0.7391\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 38/500, Training Loss: 3.6267, Validation Loss: 3.9730\n",
      "Epoch 38/500, best pearson corr. on the validation set so far: 0.7443\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 39/500, Training Loss: 3.5705, Validation Loss: 3.9622\n",
      "Epoch 39/500, best pearson corr. on the validation set so far: 0.7499\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 41/500, Training Loss: 3.5321, Validation Loss: 3.9413\n",
      "Epoch 41/500, best pearson corr. on the validation set so far: 0.7527\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 42/500, Training Loss: 3.5169, Validation Loss: 3.8807\n",
      "Epoch 42/500, best pearson corr. on the validation set so far: 0.7528\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 43/500, Training Loss: 3.4942, Validation Loss: 3.8525\n",
      "Epoch 43/500, best pearson corr. on the validation set so far: 0.7558\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 44/500, Training Loss: 3.4539, Validation Loss: 3.8588\n",
      "Epoch 44/500, best pearson corr. on the validation set so far: 0.7623\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 45/500, Training Loss: 3.4149, Validation Loss: 3.8199\n",
      "Epoch 45/500, best pearson corr. on the validation set so far: 0.7648\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 46/500, Training Loss: 3.3627, Validation Loss: 3.7832\n",
      "Epoch 46/500, best pearson corr. on the validation set so far: 0.7688\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 48/500, Training Loss: 3.2787, Validation Loss: 3.7107\n",
      "Epoch 48/500, best pearson corr. on the validation set so far: 0.7707\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 49/500, Training Loss: 3.2501, Validation Loss: 3.6864\n",
      "Epoch 49/500, best pearson corr. on the validation set so far: 0.7717\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 52/500, Training Loss: 3.2029, Validation Loss: 3.6566\n",
      "Epoch 52/500, best pearson corr. on the validation set so far: 0.7737\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 53/500, Training Loss: 3.1953, Validation Loss: 3.6701\n",
      "Epoch 53/500, best pearson corr. on the validation set so far: 0.7771\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 54/500, Training Loss: 3.2007, Validation Loss: 3.6120\n",
      "Epoch 54/500, best pearson corr. on the validation set so far: 0.7789\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 55/500, Training Loss: 3.1547, Validation Loss: 3.5925\n",
      "Epoch 55/500, best pearson corr. on the validation set so far: 0.7809\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 57/500, Training Loss: 3.0857, Validation Loss: 3.5611\n",
      "Epoch 57/500, best pearson corr. on the validation set so far: 0.7817\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 58/500, Training Loss: 3.0460, Validation Loss: 3.5468\n",
      "Epoch 58/500, best pearson corr. on the validation set so far: 0.7832\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 60/500, Training Loss: 3.0219, Validation Loss: 3.5354\n",
      "Epoch 60/500, best pearson corr. on the validation set so far: 0.7840\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 61/500, Training Loss: 2.9999, Validation Loss: 3.5280\n",
      "Epoch 61/500, best pearson corr. on the validation set so far: 0.7843\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 62/500, Training Loss: 2.9966, Validation Loss: 3.5036\n",
      "Epoch 62/500, best pearson corr. on the validation set so far: 0.7850\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 63/500, Training Loss: 2.9891, Validation Loss: 3.5155\n",
      "Epoch 63/500, best pearson corr. on the validation set so far: 0.7862\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 64/500, Training Loss: 3.0195, Validation Loss: 3.5792\n",
      "Epoch 64/500, best pearson corr. on the validation set so far: 0.7878\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 65/500, Training Loss: 3.0093, Validation Loss: 3.4748\n",
      "Epoch 65/500, best pearson corr. on the validation set so far: 0.7921\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 74/500, Training Loss: 2.8078, Validation Loss: 3.4556\n",
      "Epoch 74/500, best pearson corr. on the validation set so far: 0.7926\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 76/500, Training Loss: 2.7590, Validation Loss: 3.4216\n",
      "Epoch 76/500, best pearson corr. on the validation set so far: 0.7936\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 77/500, Training Loss: 2.7842, Validation Loss: 3.4088\n",
      "Epoch 77/500, best pearson corr. on the validation set so far: 0.7942\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 78/500, Training Loss: 2.7479, Validation Loss: 3.4103\n",
      "Epoch 78/500, best pearson corr. on the validation set so far: 0.7958\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 86/500, Training Loss: 2.6560, Validation Loss: 3.3982\n",
      "Epoch 86/500, best pearson corr. on the validation set so far: 0.7963\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 94/500, Training Loss: 2.5973, Validation Loss: 3.4116\n",
      "Epoch 94/500, best pearson corr. on the validation set so far: 0.7975\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "validation\n",
      "test\n",
      "number of epochs 500\n",
      "weight_decay 1e-05\n",
      "Epoch 1/500, Training Loss: 8.2245, Validation Loss: 7.9298\n",
      "Epoch 1/500, best pearson corr. on the validation set so far: 0.1077\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 3/500, Training Loss: 6.8656, Validation Loss: 5.9248\n",
      "Epoch 3/500, best pearson corr. on the validation set so far: 0.1347\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 4/500, Training Loss: 6.0515, Validation Loss: 6.1193\n",
      "Epoch 4/500, best pearson corr. on the validation set so far: 0.2965\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 5/500, Training Loss: 6.0736, Validation Loss: 5.6770\n",
      "Epoch 5/500, best pearson corr. on the validation set so far: 0.3372\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 6/500, Training Loss: 5.8966, Validation Loss: 5.7857\n",
      "Epoch 6/500, best pearson corr. on the validation set so far: 0.3461\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 7/500, Training Loss: 5.8431, Validation Loss: 5.6022\n",
      "Epoch 7/500, best pearson corr. on the validation set so far: 0.3529\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 11/500, Training Loss: 5.6655, Validation Loss: 5.4629\n",
      "Epoch 11/500, best pearson corr. on the validation set so far: 0.3567\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 12/500, Training Loss: 5.6346, Validation Loss: 5.4312\n",
      "Epoch 12/500, best pearson corr. on the validation set so far: 0.3642\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 13/500, Training Loss: 5.6014, Validation Loss: 5.3909\n",
      "Epoch 13/500, best pearson corr. on the validation set so far: 0.3780\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 14/500, Training Loss: 5.5581, Validation Loss: 5.3377\n",
      "Epoch 14/500, best pearson corr. on the validation set so far: 0.4012\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 15/500, Training Loss: 5.4985, Validation Loss: 5.2526\n",
      "Epoch 15/500, best pearson corr. on the validation set so far: 0.4390\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 16/500, Training Loss: 5.4058, Validation Loss: 5.1337\n",
      "Epoch 16/500, best pearson corr. on the validation set so far: 0.4800\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 17/500, Training Loss: 5.2918, Validation Loss: 5.0044\n",
      "Epoch 17/500, best pearson corr. on the validation set so far: 0.5156\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 18/500, Training Loss: 5.1730, Validation Loss: 4.8936\n",
      "Epoch 18/500, best pearson corr. on the validation set so far: 0.5393\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 19/500, Training Loss: 5.0718, Validation Loss: 4.8147\n",
      "Epoch 19/500, best pearson corr. on the validation set so far: 0.5552\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 20/500, Training Loss: 4.9852, Validation Loss: 4.7659\n",
      "Epoch 20/500, best pearson corr. on the validation set so far: 0.5663\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 21/500, Training Loss: 4.9329, Validation Loss: 4.7198\n",
      "Epoch 21/500, best pearson corr. on the validation set so far: 0.5779\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 22/500, Training Loss: 4.8619, Validation Loss: 4.6585\n",
      "Epoch 22/500, best pearson corr. on the validation set so far: 0.5924\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 23/500, Training Loss: 4.7576, Validation Loss: 4.5925\n",
      "Epoch 23/500, best pearson corr. on the validation set so far: 0.6118\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 24/500, Training Loss: 4.6924, Validation Loss: 4.5398\n",
      "Epoch 24/500, best pearson corr. on the validation set so far: 0.6337\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 25/500, Training Loss: 4.5695, Validation Loss: 4.3976\n",
      "Epoch 25/500, best pearson corr. on the validation set so far: 0.6590\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 26/500, Training Loss: 4.4248, Validation Loss: 4.2594\n",
      "Epoch 26/500, best pearson corr. on the validation set so far: 0.6768\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500, Training Loss: 4.3104, Validation Loss: 4.1954\n",
      "Epoch 27/500, best pearson corr. on the validation set so far: 0.6856\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 28/500, Training Loss: 4.2305, Validation Loss: 4.1453\n",
      "Epoch 28/500, best pearson corr. on the validation set so far: 0.6941\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 29/500, Training Loss: 4.1663, Validation Loss: 4.0922\n",
      "Epoch 29/500, best pearson corr. on the validation set so far: 0.7001\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 30/500, Training Loss: 4.1330, Validation Loss: 4.0743\n",
      "Epoch 30/500, best pearson corr. on the validation set so far: 0.7020\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 31/500, Training Loss: 4.0990, Validation Loss: 4.0414\n",
      "Epoch 31/500, best pearson corr. on the validation set so far: 0.7065\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 32/500, Training Loss: 4.0742, Validation Loss: 4.0289\n",
      "Epoch 32/500, best pearson corr. on the validation set so far: 0.7099\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 33/500, Training Loss: 4.0296, Validation Loss: 4.0320\n",
      "Epoch 33/500, best pearson corr. on the validation set so far: 0.7135\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 34/500, Training Loss: 3.9860, Validation Loss: 3.9282\n",
      "Epoch 34/500, best pearson corr. on the validation set so far: 0.7220\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 35/500, Training Loss: 3.9153, Validation Loss: 3.8550\n",
      "Epoch 35/500, best pearson corr. on the validation set so far: 0.7305\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 36/500, Training Loss: 3.8521, Validation Loss: 3.7898\n",
      "Epoch 36/500, best pearson corr. on the validation set so far: 0.7355\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 37/500, Training Loss: 3.8008, Validation Loss: 3.7564\n",
      "Epoch 37/500, best pearson corr. on the validation set so far: 0.7411\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 38/500, Training Loss: 3.7295, Validation Loss: 3.7458\n",
      "Epoch 38/500, best pearson corr. on the validation set so far: 0.7470\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 40/500, Training Loss: 3.6536, Validation Loss: 3.6885\n",
      "Epoch 40/500, best pearson corr. on the validation set so far: 0.7492\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 41/500, Training Loss: 3.6509, Validation Loss: 3.6740\n",
      "Epoch 41/500, best pearson corr. on the validation set so far: 0.7502\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 42/500, Training Loss: 3.6247, Validation Loss: 3.6945\n",
      "Epoch 42/500, best pearson corr. on the validation set so far: 0.7507\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 43/500, Training Loss: 3.6088, Validation Loss: 3.6427\n",
      "Epoch 43/500, best pearson corr. on the validation set so far: 0.7554\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 44/500, Training Loss: 3.5779, Validation Loss: 3.6307\n",
      "Epoch 44/500, best pearson corr. on the validation set so far: 0.7574\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 45/500, Training Loss: 3.5486, Validation Loss: 3.5930\n",
      "Epoch 45/500, best pearson corr. on the validation set so far: 0.7595\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 46/500, Training Loss: 3.5140, Validation Loss: 3.6015\n",
      "Epoch 46/500, best pearson corr. on the validation set so far: 0.7626\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 47/500, Training Loss: 3.4975, Validation Loss: 3.5880\n",
      "Epoch 47/500, best pearson corr. on the validation set so far: 0.7657\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 48/500, Training Loss: 3.4531, Validation Loss: 3.5442\n",
      "Epoch 48/500, best pearson corr. on the validation set so far: 0.7695\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 50/500, Training Loss: 3.3890, Validation Loss: 3.5194\n",
      "Epoch 50/500, best pearson corr. on the validation set so far: 0.7704\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 51/500, Training Loss: 3.3759, Validation Loss: 3.5224\n",
      "Epoch 51/500, best pearson corr. on the validation set so far: 0.7726\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 53/500, Training Loss: 3.3303, Validation Loss: 3.4654\n",
      "Epoch 53/500, best pearson corr. on the validation set so far: 0.7756\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 54/500, Training Loss: 3.3223, Validation Loss: 3.4421\n",
      "Epoch 54/500, best pearson corr. on the validation set so far: 0.7776\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 55/500, Training Loss: 3.2845, Validation Loss: 3.4361\n",
      "Epoch 55/500, best pearson corr. on the validation set so far: 0.7799\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 56/500, Training Loss: 3.2756, Validation Loss: 3.4071\n",
      "Epoch 56/500, best pearson corr. on the validation set so far: 0.7820\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 57/500, Training Loss: 3.2103, Validation Loss: 3.3720\n",
      "Epoch 57/500, best pearson corr. on the validation set so far: 0.7874\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 58/500, Training Loss: 3.1850, Validation Loss: 3.3500\n",
      "Epoch 58/500, best pearson corr. on the validation set so far: 0.7879\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 59/500, Training Loss: 3.1552, Validation Loss: 3.3384\n",
      "Epoch 59/500, best pearson corr. on the validation set so far: 0.7898\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 60/500, Training Loss: 3.1339, Validation Loss: 3.3321\n",
      "Epoch 60/500, best pearson corr. on the validation set so far: 0.7898\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 61/500, Training Loss: 3.1162, Validation Loss: 3.3274\n",
      "Epoch 61/500, best pearson corr. on the validation set so far: 0.7903\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 62/500, Training Loss: 3.1080, Validation Loss: 3.3217\n",
      "Epoch 62/500, best pearson corr. on the validation set so far: 0.7920\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 63/500, Training Loss: 3.0883, Validation Loss: 3.3081\n",
      "Epoch 63/500, best pearson corr. on the validation set so far: 0.7933\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 64/500, Training Loss: 3.0832, Validation Loss: 3.2862\n",
      "Epoch 64/500, best pearson corr. on the validation set so far: 0.7936\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 65/500, Training Loss: 3.0730, Validation Loss: 3.3066\n",
      "Epoch 65/500, best pearson corr. on the validation set so far: 0.7962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 66/500, Training Loss: 3.0356, Validation Loss: 3.2548\n",
      "Epoch 66/500, best pearson corr. on the validation set so far: 0.7997\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 67/500, Training Loss: 3.0008, Validation Loss: 3.2316\n",
      "Epoch 67/500, best pearson corr. on the validation set so far: 0.8031\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 69/500, Training Loss: 2.9543, Validation Loss: 3.2514\n",
      "Epoch 69/500, best pearson corr. on the validation set so far: 0.8039\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 74/500, Training Loss: 2.9128, Validation Loss: 3.2205\n",
      "Epoch 74/500, best pearson corr. on the validation set so far: 0.8039\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 75/500, Training Loss: 2.8841, Validation Loss: 3.2547\n",
      "Epoch 75/500, best pearson corr. on the validation set so far: 0.8052\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 77/500, Training Loss: 2.8934, Validation Loss: 3.2113\n",
      "Epoch 77/500, best pearson corr. on the validation set so far: 0.8073\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 78/500, Training Loss: 2.8343, Validation Loss: 3.1876\n",
      "Epoch 78/500, best pearson corr. on the validation set so far: 0.8095\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 79/500, Training Loss: 2.8141, Validation Loss: 3.1921\n",
      "Epoch 79/500, best pearson corr. on the validation set so far: 0.8096\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 81/500, Training Loss: 2.7766, Validation Loss: 3.1698\n",
      "Epoch 81/500, best pearson corr. on the validation set so far: 0.8104\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 85/500, Training Loss: 2.7246, Validation Loss: 3.1556\n",
      "Epoch 85/500, best pearson corr. on the validation set so far: 0.8106\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 86/500, Training Loss: 2.7598, Validation Loss: 3.2133\n",
      "Epoch 86/500, best pearson corr. on the validation set so far: 0.8115\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 90/500, Training Loss: 2.6359, Validation Loss: 3.1452\n",
      "Epoch 90/500, best pearson corr. on the validation set so far: 0.8134\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 95/500, Training Loss: 2.5866, Validation Loss: 3.1355\n",
      "Epoch 95/500, best pearson corr. on the validation set so far: 0.8144\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 98/500, Training Loss: 2.5452, Validation Loss: 3.1328\n",
      "Epoch 98/500, best pearson corr. on the validation set so far: 0.8147\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 107/500, Training Loss: 2.4208, Validation Loss: 3.1536\n",
      "Epoch 107/500, best pearson corr. on the validation set so far: 0.8158\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 128/500, Training Loss: 2.1413, Validation Loss: 3.1818\n",
      "Epoch 128/500, best pearson corr. on the validation set so far: 0.8168\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch 129/500, Training Loss: 2.1365, Validation Loss: 3.1742\n",
      "Epoch 129/500, best pearson corr. on the validation set so far: 0.8174\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500, Training Loss: 2.0882, Validation Loss: 3.1621\n",
      "Epoch 131/500, best pearson corr. on the validation set so far: 0.8177\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "===============\n",
      "run_name: run_0\n",
      "run_name: run_1\n",
      "run_name: run_2\n",
      "run_name: run_3\n",
      "run_name: run_4\n"
     ]
    }
   ],
   "source": [
    "dsettypes = ['train', 'validation','test']\n",
    "gpu_index = 0\n",
    "res_desc = {}\n",
    "version=2\n",
    "for model_name in [ 'RNN']:  #'FFN','CNN', 'RNN',Transformer\n",
    "    print(model_name)\n",
    "    args.model_name =  model_name # {'RNN','CNN', 'Transformer'}\n",
    "    res_desc[model_name] = {}\n",
    "    for exp_name in ['protospacer']: #,'protospacer_extended']:\n",
    "        args.exp_name = exp_name\n",
    "        model_path = os.path.join(args.working_dir, \n",
    "                                  'output', \n",
    "                                  f'{model_name}_v{version}',\n",
    "                                  exp_name)\n",
    "        dpartitions, datatensor_partitions = get_data_ready(args, \n",
    "                                                            normalize_opt='max',\n",
    "                                                            train_size=0.9, \n",
    "                                                            fdtype=torch.float32,\n",
    "                                                            plot_y_distrib=False)\n",
    "        mconfig, options = get_hyperparam_config(args)\n",
    "        print(options)\n",
    "        \n",
    "#         options['num_epochs'] = 10 # use this if you want to test a whole workflow run for all models using 10 epochs\n",
    "        \n",
    "        perfmetric_name = options['perfmetric_name']\n",
    "        train_val_path = os.path.join(model_path, 'train_val')\n",
    "        test_path = os.path.join(model_path, 'test')\n",
    "        \n",
    "        print(f'Running model: {model_name}, exp_name: {exp_name}, saved at {train_val_path}')\n",
    "        perfmetric_run_map, score_run_dict = run_trainevaltest_workflow(datatensor_partitions, \n",
    "                                                                        (mconfig, options), \n",
    "                                                                        train_val_path,\n",
    "                                                                        dsettypes,\n",
    "                                                                        perfmetric_name,\n",
    "                                                                        gpu_index, \n",
    "                                                                        to_gpu=True)\n",
    "        print('='*15)\n",
    "        res_desc[model_name][exp_name] = compute_eval_results_df(train_val_path, len(dpartitions)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b9f127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RNN': {'protospacer':              run_0     run_1     run_2     run_3     run_4      mean   \n",
       "  spearman  0.765320  0.786523  0.768976  0.766275  0.758588  0.769136  \\\n",
       "  pearson   0.810613  0.815035  0.795311  0.797432  0.796956  0.803069   \n",
       "  MAE       3.832806  3.877604  3.869510  3.945184  3.703842  3.845789   \n",
       "  \n",
       "              median    stddev  \n",
       "  spearman  0.766275  0.010444  \n",
       "  pearson   0.797432  0.009075  \n",
       "  MAE       3.869510  0.089123  }}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a043f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
